# ğŸ—ï¸ MVC-ML Architecture â€“ Fraud Detection System 

## ğŸ“ Project Structure

```
src/ml/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ processing/           # Reusable functions (OPTIMIZED!)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                  # OPTIMIZED: PostgreSQL COPY (81.6% faster)
â”‚   â”œâ”€â”€ validation.py              # Schema and data integrity validation
â”‚   â”œâ”€â”€ cleaning.py                # Outlier analysis (preserves all data)
â”‚   â”œâ”€â”€ normalization.py           # RobustScaler + StandardScaler âœ…
â”‚   â”œâ”€â”€ feature_engineering.py     # 9 new features (Time_Period, Amount_Bin, etc.) âœ…
â”‚   â”œâ”€â”€ feature_selection.py       # Automated analysis (Pearson, Spearman, MI, VIF) âœ…
â”‚   â”œâ”€â”€ splitters.py               # Stratified train/test split âœ…
â”‚   â””â”€â”€ metadata.py                # Pipeline metadata tracking
â”‚
â”œâ”€â”€ pipelines/            # Scripts chaining functions  
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_pipeline.py           # Complete pipeline (Steps 01â€“07) âœ…
â”‚                                   # - PostgreSQL COPY for bulk inserts
â”‚                                   # - Parallel Steps 02â€“03 (ThreadPoolExecutor)
â”‚                                   # - Metadata-only mode for non-mutating steps
â”‚                                   # - Feature Selection Analysis (Step 05.5)
â”‚                                   # - Config-driven feature removal (Step 06)
â”‚
â”œâ”€â”€ models/               # Configurations and schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ configs.py                 # Centralized configuration (dataclasses)
â”‚                                   # - Loads hyperparameters from data/xgboost_hyperparameters.json
â”‚                                   # - Defines feature selection (excluded_features)
â”‚                                   # - Config-driven approach (no hardcoded values)
â”‚
â”œâ”€â”€ training/             # Training scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                   # Training using hyperparameters from JSON
â”‚   â”œâ”€â”€ tune.py                    # Grid Search + automatic JSON update
â”‚   â””â”€â”€ archive/                   # Deprecated scripts (history)
```

---

## ğŸ¯ Architecture Principles

### 1. **Separation of Concerns**

* **Processing** â€“ Pure, reusable, testable functions
* **Pipelines** â€“ Orchestrate processing steps
* **Models** â€“ Configs and schemas loaded from JSON
* **Training** â€“ Model training and tuning

### 2. **Modularity**

* Each function in `processing/` does **exactly one thing**
* Easy to test in isolation
* Simple to compose into different pipelines

### 3. **Centralized Configuration + Versioned JSON**

* Hyperparameters in `data/xgboost_hyperparameters.json` (versioned)
* Feature selection in `models/configs.py` (hardcoded but documented) 
* Easy to change without touching code
* Automatic archival of previous versions in `data/archive/`

### 4. **Production-Ready**

* No hardcoded comparison values
* Dynamic baselines loaded from `data/archive/`
* Automatic versioning of models and hyperparameters

### 5. **Optimized Performance** âš¡

* PostgreSQL COPY for bulk inserts (70â€“80% faster)
* Parallelization of independent steps (ThreadPoolExecutor)
* Metadata tracking prevents data duplication
* Full pipeline runtime: **~62 s** (previously ~130 s) â†’ **52% faster!**

---

## ğŸš€ Usage

### Run the Full Pipeline

```bash
# Direct execution
python -m src.ml.pipelines.data_pipeline
```

### Import Functions in Your Scripts

```python
# Load data (OPTIMIZED with COPY!)
from src.ml.processing.loader import load_from_postgresql, save_to_postgresql

# Normalize (RobustScaler for outliers)
from src.ml.processing.normalization import fit_and_transform_features

# Feature engineering (9 new features)
from src.ml.processing.feature_engineering import engineer_all_features

# Stratified train/test split
from src.ml.processing.splitters import stratified_train_test_split

# Use configuration
from src.ml.models.configs import config
print(config.database.connection_string)
```

---

## ğŸ“Š Data Pipeline (7 Steps) â€“ COMPLETE âœ…

### Step 01 â€“ Load Raw Data âœ…

* **Input:** `data/creditcard.csv`
* **Output:** PostgreSQL `raw_transactions` (284,807 rows)
* **Functions:** `processing/loader.py::load_csv_to_dataframe()` + `save_to_postgresql()`
* **Performance:** ~20 s (optimized COPY)

### Step 02 â€“ Outlier Analysis âœ…

* **Input:** `raw_transactions`
* **Output:** **Metadata only** (no data duplication)
* **Function:** `processing/cleaning.py::identify_outliers()`
* **Decision:** Preserve 100% of data (18.5% frauds among outliers)
* **Performance:** ~2.4 s
* **Parallelization:** Runs concurrently with Step 03 âš¡

### Step 03 â€“ Handle Missing âœ…

* **Input:** `raw_transactions`
* **Output:** **Metadata only**
* **Function:** `processing/validation.py::validate_no_missing_values()`
* **Result:** No missing values found (dataset fully complete)
* **Performance:** ~2.4 s
* **Parallelization:** Runs concurrently with Step 02 âš¡

### Step 04 â€“ Normalize âœ…

* **Input:** `raw_transactions`
* **Output:** `normalized_transactions` + `models/scalers.pkl`
* **Function:** `processing/normalization.py::fit_and_transform_features()`
* **Scalers:** RobustScaler (Amount), StandardScaler (Time)
* **Performance:** ~16.5 s (was ~90 s) â†’ **81.6% faster!** ğŸ”¥
* **Optimization:** PostgreSQL COPY instead of `to_sql()`
* **Versioning:** Optional â€“ `models/scalers_v1.0.0.pkl`

### Step 05 â€“ Feature Engineering âœ…

* **Input:** `normalized_transactions`
* **Output:** `engineered_transactions` (284,807 Ã— 40 cols)
* **Function:** `processing/feature_engineering.py::engineer_all_features()`
* **Features:** Time_Period, Amount_Log, Amount_Bin, V-statistics (Mean/Std/Min/Max/Range)
* **Performance:** ~20.6 s (optimized COPY)

### Step 05.5 â€“ Feature Selection Analysis âœ…

* **Input:** `engineered_transactions`
* **Output:** `reports/feature_selection_analysis.json` (JSON report)
* **Function:** `processing/feature_selection.py::analyze_feature_importance()`
* **Analyses:** Pearson, Spearman, Mutual Information, VIF, Correlation Matrix
* **Performance:** ~15 s
* **Action:** Review report and update `configs.py`

### Step 06 â€“ Apply Feature Selection âœ…

* **Input:** `engineered_transactions`
* **Output:** `selected_features` (if configured)
* **Function:** `processing/feature_selection.py::apply_feature_selection()`
* **Config-Driven:** Removes features listed in `configs.py::FeatureSelectionConfig.excluded_features`
* **Performance:** ~5 s (or metadata-only if empty)
* **Versioning:** Increment `model_version` when changes occur

### Step 07 â€“ Train/Test Split âœ…

* **Input:** `selected_features` or `engineered_transactions`
* **Output:** `train_data` (227 845) + `test_data` (56 962)
* **Function:** `processing/splitters.py::stratified_train_test_split()`
* **Split:** 80/20 stratified (keeps 0.172% ratio)
* **Performance:** ~22.2 s (using COPY for both tables)

---

## ğŸ¯ Feature Selection Strategy â€“ Config-Driven

### Philosophy: Automated Analysis + Manual Decision

**Why not fully automate?**

* âŒ Automatic removal may discard non-linear but useful features
* âŒ Business context matters (e.g. high-value frauds, temporal patterns)
* âŒ Manual A/B testing gives better control

**Solution: Config-Driven Approach**

1. âœ… Step 05.5 â€“ Run automated analysis (Pearson, Spearman, MI, VIF)
2. âœ… JSON Report â€“ `reports/feature_selection_analysis.json`
3. âœ… Developer reviews the report
4. âœ… Update `configs.py` â†’ `excluded_features`
5. âœ… Step 06 â€“ Automatically applies removal on next run

### Development Workflow

*(The step-by-step commands remain unchanged; they are already in English in code.)*

---

### Benefits of This Approach

| Aspect               | Advantage                                           |
| -------------------- | --------------------------------------------------- |
| **Traceability**     | Git tracks feature-removal decisions (`configs.py`) |
| **Reproducibility**  | Same code + same config â†’ same results              |
| **A/B Testing**      | Toggle features easily                              |
| **Business Context** | Human oversight before removal                      |
| **Production-Ready** | Mirrors enterprise config management                |
| **Educational**      | JSON report explains feature importance             |

---

## ğŸ”– Model Versioning

### Scalers (Pickle)

**MVP/POC (current)**

```python
models/scalers.pkl  # Active version
```

**Production (recommended)**

```python
models/
  scalers_v1.0.0.pkl
  scalers_v1.1.0.pkl  # With feature selection applied
  scalers_v1.2.0.pkl  # With new features added
```

**Enterprise**

```python
import mlflow
with mlflow.start_run():
    mlflow.sklearn.log_model(scaler, "scaler")
    mlflow.log_param("scaler_type", "RobustScaler")
    mlflow.log_param("model_version", "1.1.0")
```

**Why Pickle for Scalers?**

| Method     | Pros                                 | Cons                     | Use                   |
| ---------- | ------------------------------------ | ------------------------ | --------------------- |
| **Pickle** | âœ… Fast, native, keeps object methods | âŒ Python-only / insecure | âœ… MVP/POC             |
| **ONNX**   | âœ… Cross-platform                     | âŒ Complex conversion     | Multi-lang production |
| **MLflow** | âœ… Full version tracking + UI         | âŒ Extra infra needed     | âœ… Enterprise          |
| **SQL**    | âœ… Centralized                        | âŒ Loses object methods   | âŒ Not recommended     |

Decision: Use Pickle + manual versioning (MVP); document MLflow path for future upgrade.

---

## âš™ï¸ Configurations

All configs live in `src/ml/models/configs.py`,
covering Database, Paths, Data, Pipeline, and Feature Selection.

---

## âš¡ Implemented Optimizations

### 1. PostgreSQL COPY (81.6% faster) ğŸ”¥

```python
def save_to_postgresql(df, engine, table_name, use_copy=True):
    """
    Uses native PostgreSQL COPY for bulk inserts.
    Automatically falls back to to_sql() if COPY fails.
    """
```

### 2. Parallel Steps 02-03 (+18.6% faster) âš¡

```python
with ThreadPoolExecutor(max_workers=2) as executor:
    future_02 = executor.submit(run_step_02_outlier_analysis)
    future_03 = executor.submit(run_step_03_handle_missing)
```

### 3. Metadata-Only Mode (ZERO unnecessary I/O)

Steps 02-03 store metadata only (no data duplication).

### 4. Overall Performance

| Metric             | Before     | After     | Gain        |
| ------------------ | ---------- | --------- | ----------- |
| Step 04            | 90 s       | 16.5 s    | **+81.6%**  |
| Steps 02-03        | 4.8 s      | 3.9 s     | **+18.6%**  |
| **Total Pipeline** | **~130 s** | **~62 s** | **+52% ğŸš€** |

---

## ğŸ§ª Tests

```bash
python test_pipeline.py
python -c "from src.ml.pipelines.data_pipeline import DataPipeline; print('OK')"
python -c "from src.ml.processing.loader import save_to_postgresql; print('OK')"
python -c "from src.ml.processing.feature_engineering import engineer_all_features; print('OK')"
```

---

## ğŸ“ˆ Next Steps

1. âœ… Data Pipeline (01â€“07) â€“ **Complete**
2. âœ… Performance Optimizations â€“ **Complete**
3. âœ… Feature Selection Analysis â€“ **Complete**
4. â³ Training Pipeline â€“ in progress (Decision Tree, SVM, XGBoost)
5. â³ Validators â€“ metrics, K-Fold, model comparison
6. â³ Flask Dashboard â€“ real-time simulation and visuals
7. â³ [Optional] Kafka Enhancement (`plan_kafka.md`)

---

## ğŸ“š References

* **MVC Pattern** â€“ Model-View-Controller
* **ML Pipelines** â€“ Modular and reproducible
* **SOLID Principles** â€“ Single Responsibility, Open/Closed
* **PostgreSQL COPY** â€“ native bulk insert (70â€“80% faster)
* **ThreadPoolExecutor** â€“ I/O-bound parallelism
* **RobustScaler** â€“ Outlier-resistant normalization

---

**Current Status:** âœ… **Full Data Pipeline Optimized (01â€“07)**
**Performance:** 52% faster (~62 s vs ~130 s)
**Feature Selection:** Config-driven, 38 final features
**Versioning:** Scalers with version support (`get_versioned_scaler_path`)
**Next:** â³ Train ML models (Decision Tree, SVM RBF, XGBoost)

### ğŸ¯ Modeling Strategy

**Phase 1 â€“ Baseline Training (38 features)**
â€“ Decision Tree, SVM RBF, XGBoost
â€“ Stratified K-Fold (k = 5)
â€“ Metrics: PR-AUC, Recall, F1, Precision

**Phase 2 â€“ Feature Importance Analysis**
â€“ Use `feature_importances_` from XGBoost
â€“ Remove low-importance features
â€“ A/B test results

**Phase 3 â€“ Refinement**
â€“ Grid Search (`scoring='average_precision'`)
â€“ Threshold tuning (~0.2â€“0.3 expected)

**Phase 4 â€“ Flask Dashboard (MVP)**
â€“ Interactive UI with real-time simulation (WebSocket streaming)

---
 