# üìä EDA Report - Fraud Detection Pipeline

**Data**: 2025-10-02  
**Pipeline**: `src/ml/processing/` (Arquitetura MVC-ML otimizada)  
**Dataset**: creditcard.csv (284,807 transa√ß√µes, 492 fraudes)

---

## üéØ Objetivo da EDA

Entender padr√µes de fraude e caracter√≠sticas do dataset para tomar **decis√µes t√©cnicas fundamentadas** no pipeline de dados.

---

## üìà 1. An√°lise de Desbalanceamento de Classes

### Distribui√ß√£o
```
Classe 0 (Leg√≠tima): 284,315 (99.828%)
Classe 1 (Fraude):       492 ( 0.172%)
Ratio: 1:578
```

### üéØ Decis√£o T√©cnica
**N√ÉO usar SMOTE ou Undersampling**

**Justificativa**:
- ‚ùå **SMOTE**: Infla dataset artificialmente, risco de overfitting
- ‚ùå **Undersampling**: Perde 99.6% dos dados leg√≠timos

**Solu√ß√£o Adotada**:
- ‚úÖ `class_weight='balanced'` (Decision Tree, SVM)
- ‚úÖ `scale_pos_weight=577` (XGBoost)
- ‚úÖ **Threshold Tuning** p√≥s-treino (0.2-0.3 em vez de 0.5)
- ‚úÖ Priorizar **Recall** > Precision (melhor bloquear leg√≠tima que deixar fraude passar)
- ‚ùå **Regress√£o Log√≠stica removida**: Sens√≠vel a outliers (assume linearidade e distribui√ß√£o normal)

---

## üí∞ 2. An√°lise de Amount (Valores de Transa√ß√£o)

### Estat√≠sticas Gerais
```
M√©trica          Leg√≠timas    Fraudes
M√©dia            $88.29       $122.21
Mediana          $22.00       $9.25
Q1 (25%)         $5.65        $1.00
Q3 (75%)         $77.17       $105.89
M√°ximo           $25,691.16   $2,125.87
```

### üìä Distribui√ß√£o de Fraudes por Faixa
```
Amount < $100:      362 fraudes (73.6%)
$100 - $500:         95 fraudes (19.3%)
$500 - $1000:        26 fraudes ( 5.3%)
$1000 - $15k:         9 fraudes ( 1.8%)
> $15k:               0 fraudes ( 0.0%)
```

### üîç An√°lise de Outliers (IQR Method)

**M√©todo Tukey (IQR)**:
```
Q1 = $5.60
Q3 = $77.16
IQR = $71.56
Lower Bound = Q1 - 1.5*IQR = -$101.75
Upper Bound = Q3 + 1.5*IQR = $184.51

Outliers detectados: 31,904 transa√ß√µes (11.2%)
  - Outliers baixos (< -$101.75): 0
  - Outliers altos (> $184.51): 31,904
```

**Fraudes em Outliers**:
```
Total de fraudes: 492
Fraudes em outliers: 91 (18.5%)
Fraudes preservadas se remover: 401 (81.5%)
```

### üéØ DECIS√ÉO CR√çTICA: N√ÉO REMOVER OUTLIERS!

**An√°lise**:
- ‚úÖ Dataset tem apenas **492 fraudes** (j√° extremamente desbalanceado)
- ‚úÖ **91 fraudes (18.5%)** est√£o em outliers (Amount > $184.51)
- ‚úÖ Estas s√£o fraudes de **ALTO VALOR** (padr√£o diferente, mas v√°lido)
- ‚úÖ Remover = perder capacidade de detectar fraudes > $180
- ‚úÖ IQR √© m√©todo cient√≠fico, mas **contexto importa mais que f√≥rmula**

**Justificativa Estat√≠stica**:
1. **Cada fraude √© PRECIOSA**: Dataset pequeno, informa√ß√£o rara
2. **Fraudes de alto valor existem**: N√£o s√£o erros, s√£o padr√µes reais
3. **Perda inaceit√°vel**: 18.5% das fraudes = 91 exemplos perdidos
4. **Alternativas melhores**: RobustScaler + modelos n√£o-lineares

**Solu√ß√£o Adotada**:
- ‚úÖ **Manter TODAS as transa√ß√µes** (incluindo outliers)
- ‚úÖ **Step 04**: Usar `RobustScaler` para Amount (resistente a outliers)
- ‚úÖ **Modelos n√£o-lineares**: XGBoost e Decision Tree lidam bem com outliers
- ‚úÖ **Threshold tuning**: Otimizar para capturar fraudes raras (valores extremos)

**Storytelling**:
> "Analisamos outliers usando IQR (m√©todo Tukey) e identificamos 31.904 transa√ß√µes extremas. No entanto, 91 fraudes (18.5% do total) est√£o nesta faixa. Como o dataset j√° tem apenas 492 fraudes, decidimos MANTER todos os dados. Cada fraude carrega informa√ß√£o preciosa sobre padr√µes de comportamento fraudulento. Nossa solu√ß√£o: usar RobustScaler (resistente a outliers) e modelos n√£o-lineares (XGBoost) que capturam rela√ß√µes complexas sem precisar descartar dados valiosos."

---

## üîó 3. An√°lise de Correla√ß√µes com Target (Class)

### M√©todos Utilizados
1. **Pearson** (correla√ß√£o linear)
2. **Spearman** (correla√ß√£o monot√¥nica)
3. **Mutual Information** (rela√ß√µes n√£o-lineares gerais)

### Top 10 Features (Ordenadas por Correla√ß√£o M√©dia)

| Rank | Feature | Pearson (Linear) | Spearman (Monot√¥nica) | Mutual Info (Geral) | M√©dia |
|------|---------|------------------|-----------------------|---------------------|-------|
| 1    | V14     | 0.3025           | 0.0646                | 0.0081              | 0.1251|
| 2    | V17     | 0.3265           | 0.0424                | 0.0060              | 0.1250|
| 3    | V12     | 0.2606           | 0.0629                | 0.0076              | 0.1104|
| 4    | V10     | 0.2169           | 0.0596                | 0.0075              | 0.0947|
| 5    | V16     | 0.1965           | 0.0499                | 0.0061              | 0.0842|
| 6    | V11     | 0.1549           | 0.0601                | 0.0068              | 0.0740|
| 7    | V3      | 0.1930           | 0.0382                | 0.0047              | 0.0786|
| 8    | V7      | 0.1873           | 0.0395                | 0.0048              | 0.0772|
| 9    | V18     | 0.1115           | 0.0303                | 0.0038              | 0.0485|
| 10   | V4      | 0.1334           | 0.0343                | 0.0040              | 0.0572|

### Bottom 10 Features (Candidatos a An√°lise)

| Rank | Feature | Pearson | Spearman | Mutual Info | M√©dia  |
|------|---------|---------|----------|-------------|--------|
| 26   | V13     | 0.0046  | 0.0037   | 0.0004      | 0.0029 |
| 27   | V15     | 0.0042  | 0.0028   | 0.0003      | 0.0024 |
| 28   | V22     | 0.0074  | 0.0065   | 0.0007      | 0.0049 |
| 29   | Amount  | 0.0056  | 0.0083   | 0.0014      | 0.0051 |
| 30   | Time    | 0.0123  | 0.0117   | 0.0019      | 0.0086 |

### üéØ Decis√£o: Remover V13,V15, V22 com Baixa Correla√ß√£o Linear

**Por qu√™?**

#### ‚ùå Correla√ß√£o Linear Baixa ‚â† Feature In√∫til!

| Cen√°rio | Correla√ß√£o Linear | Utilidade no ML |
|---------|-------------------|-----------------|
| Rela√ß√£o n√£o-linear | Baixa (0.005) | ‚úÖ **ALTA!** (XGBoost captura) |
| Intera√ß√µes com outras features | Baixa individual | ‚úÖ **ALTA!** (Amount √ó Time) |
| Import√¢ncia por faixa | Baixa no geral | ‚úÖ **M√âDIA** (fraudes >$500 t√™m padr√£o) |
| Redund√¢ncia com PCA | Baixa | ‚ùå **BAIXA** (V1-V28 j√° capturam) |

#### Exemplo: Amount e Time

**Amount**:
- Correla√ß√£o Pearson: **0.0056** (quase zero!)
- Correla√ß√£o Spearman: **0.0083** (ainda baix√≠ssima)
- Mutual Info: **0.0014** (quase nenhuma rela√ß√£o)

**MAS**:
- ‚úÖ 73.6% das fraudes t√™m Amount < $100 (padr√£o claro!)
- ‚úÖ Feature engineering: `Amount_Bin` (categoriza√ß√£o) pode revelar padr√µes
- ‚úÖ Intera√ß√£o com Time: `Amount √ó Time_Period` (fraudes altas √† noite?)
- ‚úÖ XGBoost pode descobrir splits n√£o-lineares

**Time**:
- Correla√ß√£o linear baixa (0.0123)
- **MAS**: Taxa de fraude varia **10x** ao longo do dia!
- Feature engineering: `Time_Period` (manh√£/tarde/noite/madrugada)

### üí° Estrat√©gias Corretas

1. **Feature Engineering**: 
   - `Time_Period` (binning temporal)
   - `Amount_Bin` (categoriza√ß√£o de valores)
   - `Amount_Log` (log-transform para normalizar distribui√ß√£o)
   - Estat√≠sticas V-features (Mean, Std, Min, Max, Range)

2. **Modelos N√£o-Lineares**: 
   - XGBoost, Random Forest capturam padr√µes complexos
   - Decision Tree faz splits autom√°ticos (n√£o precisa de correla√ß√£o linear)

3. **Feature Importance**: 
   - Deixar o modelo decidir (n√£o descartar a priori)
   - Usar `feature_importances_` ap√≥s treino

4. **Threshold Tuning**: 
   - Otimizar decis√£o final (0.2-0.3 em vez de 0.5)
   - Priorizar Recall (capturar fraudes mesmo com baixa confian√ßa)

---

## ‚è∞ 4. An√°lise Temporal

### Distribui√ß√£o de Transa√ß√µes ao Longo do Tempo
- Dataset cobre **2 dias** (172.800 segundos)
- Transa√ß√µes concentradas em **hor√°rios comerciais**
- Gaps noturnos (per√≠odos sem transa√ß√µes)

### Taxa de Fraude ao Longo do Tempo
```
Hor√°rio (aproximado)    Taxa de Fraude
00:00 - 06:00           ~1.2% (PICO!)
06:00 - 12:00           ~0.1%
12:00 - 18:00           ~0.1%
18:00 - 24:00           ~0.3%
```

### üéØ Insight Cr√≠tico
**Taxa de fraude √© 10x maior √† madrugada!**

**Implica√ß√µes**:
- ‚úÖ Feature engineering: `Time_Period` (categorizar por per√≠odo do dia)
- ‚úÖ Modelo pode ajustar threshold dinamicamente (hor√°rio)
- ‚úÖ Em produ√ß√£o: sistema pode ser mais agressivo √† noite

---

## üìä 5. Pipeline de Dados - Decis√µes T√©cnicas (ATUALIZADO - OTIMIZADO)

### Step 01: Load Raw Data ‚úÖ
- **Input**: `data/creditcard.csv`
- **Output**: PostgreSQL `raw_transactions` (284,807 linhas)
- **Valida√ß√£o**: Schema (31 colunas), integridade
- **Performance**: ~20s (usando PostgreSQL COPY otimizado)
- **Fun√ß√£o**: `src/ml/processing/loader.py::save_to_postgresql()` (com use_copy=True)

### Step 02: An√°lise de Outliers (SEM Remo√ß√£o) ‚úÖ
- **Entrada**: `raw_transactions`
- **Sa√≠da**: **Metadata apenas** (dados N√ÉO duplicados!)
- **Fun√ß√£o**: `src/ml/processing/cleaning.py::identify_outliers()`
- **M√©todo**: IQR (Tukey)
- **Outliers detectados**: 31,904 (11.2%)
- **Fraudes em outliers**: 91 (18.5% do total)
- **DECIS√ÉO**: ‚ùå **N√ÉO REMOVER** (manter todas as fraudes, sen√£o pode n√£o detectar fraudes de alto valor)
- **Solu√ß√£o**: RobustScaler no Step 04
- **Performance**: ~2.4s
- **Impacto nos Modelos**:
  - ‚úÖ **Decision Tree**: Robusto a outliers (splits baseados em ranking)
  - ‚úÖ **SVM**: RobustScaler normaliza, kernel RBF captura n√£o-linearidades
  - ‚úÖ **XGBoost**: Extremamente robusto (√°rvores + regulariza√ß√£o)
  - ‚ùå **Regress√£o Log√≠stica REMOVIDA**: Assume linearidade, muito sens√≠vel a outliers

### Step 03: Handle Missing Values ‚úÖ
- **Entrada**: `raw_transactions`
- **Sa√≠da**: **Metadata apenas** (dados n√£o tem missing values!)
- **Fun√ß√£o**: `src/ml/processing/validation.py::validate_no_missing_values()`
- **Resultado**: Nenhum missing value encontrado (dataset 100% completo)
- **Performance**: ~2.4s
- **Paraleliza√ß√£o**: Roda em paralelo com Step 02 (ThreadPoolExecutor)
- **Estrat√©gias** (caso houvesse missing):
  - Class: Remover linha (cr√≠tico)
  - Time: Imputar com mediana
  - Amount: Imputar com mediana por classe
  - V1-V28: Imputar com mediana

### Step 04: Normalize Features ‚úÖ **[NOVO - OTIMIZADO]**
- **Entrada**: `raw_transactions` (Steps 02-03 n√£o modificaram dados)
- **Sa√≠da**: `normalized_transactions` + `models/scalers.pkl`
- **Fun√ß√£o**: `src/ml/processing/normalization.py::fit_and_transform_features()`
- **Scalers**: 
  - **RobustScaler** para Amount (resistente a outliers - usa mediana e IQR)
  - **StandardScaler** para Time (distribui√ß√£o normal)
  - **V1-V28**: Mantidos (j√° s√£o PCA, normalizados)
- **Performance**: ~16.5s (otimizado com PostgreSQL COPY - antes era 90s!)
- **Justificativa RobustScaler**: 
  - Usa mediana e IQR (n√£o m√©dia/desvio)
  - N√£o afetado por outliers
  - Preserva 100% dos dados
  - Ideal para fraudes de alto valor

### Step 05: Feature Engineering ‚úÖ **[NOVO]**
- **Entrada**: `normalized_transactions`
- **Sa√≠da**: `engineered_transactions` (284,807 linhas √ó 40 colunas)
- **Fun√ß√£o**: `src/ml/processing/feature_engineering.py::engineer_all_features()`
- **Performance**: ~20.6s (com PostgreSQL COPY otimizado)
- **Features Criadas** (9 novas):
  1. **Time_Period**: Categoriza√ß√£o temporal (4 bins)
     - Madrugada (0-6h), Manh√£ (6-12h), Tarde (12-18h), Noite (18-24h)
     - Captura padr√£o: fraudes 10x mais frequentes √† madrugada
  
  2. **Amount_Log**: Log transformation de Amount
     - Normaliza distribui√ß√£o assim√©trica
     - Reduz impacto de valores extremos
  
  3. **Amount_Bin**: Categoriza√ß√£o de valores (4 bins)
     - <$10, $10-100, $100-500, >$500
     - Captura padr√£o: 73.6% fraudes < $100
  
  4. **V_Features_Mean**: M√©dia de V1-V28
     - Feature agregada para capturar tend√™ncia central
  
  5. **V_Features_Std**: Desvio padr√£o de V1-V28
     - Captura variabilidade entre features PCA
  
  6. **V_Features_Min**: M√≠nimo de V1-V28
     - Captura valores extremos negativos
  
  7. **V_Features_Max**: M√°ximo de V1-V28
     - Captura valores extremos positivos
  
  8. **V_Features_Range**: Range (Max - Min) de V1-V28
     - Captura amplitude de varia√ß√£o
  
  9. **(Opcional) V-Interactions**: Intera√ß√µes entre top features
     - V17√óV14, V17√óV12, V14√óV12
     - Captura rela√ß√µes n√£o-lineares

- **Decis√£o de Design**:
  - N√£o criar intera√ß√µes por padr√£o (explos√£o combinat√≥ria)
  - Deixar modelos n√£o-lineares (XGBoost) descobrirem
  - Manter simplicidade e interpretabilidade

### Step 06: Train/Test Split ‚úÖ **[NOVO]**
- **Entrada**: `engineered_transactions` (284,807 linhas √ó 40 colunas)
- **Sa√≠da**: `train_data` (227,845 linhas) + `test_data` (56,962 linhas)
- **Fun√ß√£o**: `src/ml/processing/splitters.py::stratified_train_test_split()`
- **Performance**: ~22.2s (com PostgreSQL COPY para ambas tabelas)
- **M√©todo**: **StratifiedShuffleSplit** (mant√©m propor√ß√£o de classes)
- **Split**: 80/20
- **Valida√ß√µes**:
  - ‚úÖ **Zero data leakage**: √çndices √∫nicos (sem overlap)
  - ‚úÖ **Propor√ß√µes preservadas**: 
    - Train: 394 fraudes (0.173%)
    - Test: 98 fraudes (0.172%)
    - Diferen√ßa: 0.0009% (praticamente id√™ntico!)
  - ‚úÖ **random_state=42**: Reprodutibilidade garantida
- **Armazenamento**: 
  - PostgreSQL `train_data` e `test_data` tables
  - Rastreabilidade completa (metadata com contagens)

---

## üöÄ 6. Otimiza√ß√µes de Performance Implementadas

### PostgreSQL COPY (81.6% mais r√°pido)
- **Antes**: `pandas.to_sql()` - 90s para Step 04
- **Depois**: PostgreSQL COPY - 16.5s para Step 04
- **Ganho**: 73.5s economizados (81.6% redu√ß√£o!)
- **Implementa√ß√£o**: `src/ml/processing/loader.py::save_to_postgresql()`
- **Fallback**: to_sql() autom√°tico se COPY falhar

### Paraleliza√ß√£o Steps 02-03 (18.6% mais r√°pido)
- **Antes**: Sequencial - 4.79s (2.43s + 2.36s)
- **Depois**: Paralelo - 3.90s (max(2.43s, 2.36s))
- **Ganho**: 0.89s economizados
- **Implementa√ß√£o**: ThreadPoolExecutor em `data_pipeline.py`
- **Seguran√ßa**: Ambos read-only, sem race conditions

### Metadata-Only para Steps sem Altera√ß√£o
- **Steps 02-03**: Salvam apenas metadata, N√ÉO duplicam dados
- **Benef√≠cio**: 
  - Zero overhead de I/O desnecess√°rio
  - Rastreabilidade mantida (tabela `pipeline_metadata`)
  - Dados permanecem em `raw_transactions`

### Performance Total do Pipeline
- **Antes otimiza√ß√£o**: ~130s (Steps 01-04)
- **Depois otimiza√ß√£o**: ~62s (Steps 01-06 completo!)
- **Ganho Total**: **52% mais r√°pido** üöÄ

---

## üîç 8. Feature Selection Strategy (Config-Driven)

### Por que Correla√ß√£o Linear Baixa ‚â† Feature In√∫til?

Durante a EDA, identificamos features com correla√ß√£o Pearson/Spearman extremamente baixa:
- V13: 0.0046 (Pearson), 0.0037 (Spearman)
- V15: 0.0042 (Pearson), 0.0028 (Spearman)
- V22: 0.0074 (Pearson), 0.0065 (Spearman)

**Decis√£o**: N√ÉO descartar automaticamente!

### Raz√µes para Manter Features com Baixa Correla√ß√£o Linear

1. **Rela√ß√µes N√£o-Lineares**: XGBoost e Decision Tree capturam padr√µes que correla√ß√£o linear n√£o detecta
2. **Intera√ß√µes**: Feature A √ó Feature B pode ser discriminativa mesmo que A e B individualmente n√£o sejam
3. **Import√¢ncia por Faixa**: Feature pode ser importante apenas em valores extremos (outliers)
4. **Contexto de Neg√≥cio**: Cada fraude √© preciosa (apenas 492 no dataset)

### Abordagem Implementada: An√°lise Automatizada + Decis√£o Manual

#### Step 05.5: Feature Selection Analysis
**Objetivo**: Informar decis√£o (N√ÉO automatizar remo√ß√£o)

**An√°lises Realizadas**:
1. **Pearson Correlation**: Rela√ß√µes lineares
2. **Spearman Correlation**: Rela√ß√µes monot√¥nicas
3. **Mutual Information**: Rela√ß√µes n√£o-lineares gerais
4. **VIF (Variance Inflation Factor)**: Multicolinearidade entre features
5. **Correlation Matrix**: Redund√¢ncia (features com correla√ß√£o > 0.95)

**Output**: `reports/feature_selection_analysis.json`

```json
{
  "total_features": 40,
  "candidates_for_removal": {
    "low_correlation_all_methods": ["V13", "V15", "V22"],
    "count": 3,
    "features_detail": {
      "V13": {
        "pearson": 0.0046,
        "spearman": 0.0037,
        "mutual_info": 0.0004
      }
    }
  },
  "multicollinearity": {
    "high_vif_features": [],
    "redundant_pairs": []
  },
  "recommendation": {
    "action": "Review and update configs.py",
    "suggested_removals": ["V13", "V15", "V22"]
  }
}
```

#### Step 06: Apply Feature Selection (Config-Driven)
**Objetivo**: Aplicar decis√µes manuais automaticamente

**Configura√ß√£o**:
```python
# src/ml/models/configs.py
@dataclass
class FeatureSelectionConfig:
    excluded_features: List[str] = field(default_factory=lambda: [
        # 'V13',  # Baixa correla√ß√£o (decis√£o: 2025-10-02)
        # 'V15',  # Baixa correla√ß√£o (decis√£o: 2025-10-02)
        # 'V22',  # Baixa correla√ß√£o (decis√£o: 2025-10-02)
    ])
    
    model_version: str = "1.0.0"  # Incrementar ao alterar lista
```

**Workflow**:
1. Executar pipeline ‚Üí gera relat√≥rio JSON
2. Revisar relat√≥rio + considerar contexto de neg√≥cio
3. Editar `configs.py` (descomentar features a remover)
4. Incrementar `model_version`
5. Re-executar pipeline ‚Üí Step 06 aplica remo√ß√£o
6. Treinar modelos ‚Üí comparar performance
7. Se melhor: commit configs; Se pior: reverter

### Vantagens da Abordagem Config-Driven

| Aspecto | Vantagem |
|---------|----------|
| **Rastreabilidade** | Git versiona decis√µes |
| **Reprodutibilidade** | Mesma config = mesmo resultado |
| **A/B Testing** | F√°cil comentar/descomentar features |
| **Context-Aware** | Considera contexto de neg√≥cio |
| **Educativo** | Relat√≥rio ensina sobre feature importance |
| **Produ√ß√£o-Ready** | Abordagem usada em empresas reais |

### Quando Remover Features?

‚úÖ **Candidatos Fortes**:
- Baixos em TODOS os 3 m√©todos (Pearson, Spearman, MI)
- VIF > 10 (multicolinearidade alta)
- Correla√ß√£o > 0.95 com outra feature (redund√¢ncia)
- An√°lise de neg√≥cio confirma irrelev√¢ncia

‚ùå **Manter**:
- Baixa correla√ß√£o linear MAS alta MI (n√£o-linear!)
- Baixa correla√ß√£o individual MAS participa de intera√ß√µes
- Contexto de neg√≥cio indica relev√¢ncia (ex: Amount, Time)
- Dataset pequeno (cada feature pode ajudar)

### Versionamento de Modelos

**Pickle para Scalers**:
```python
# models/scalers_v1.1.0.pkl (ap√≥s feature selection)
# Versionamento manual controlado por configs.py
```

---

## üéØ 9. Estrat√©gia de Modelagem - MVP Pragm√°tico

### Fase 1: Treinamento Inicial (39 Features)

**Decis√£o**: Come√ßar com **39 features** (Time removido, Amount mantido)

**Justificativa**:
- ‚úÖ **Pragmatismo**: Deixar modelos decidirem quais features importam
- ‚úÖ **Feature Engineering Completo**: 9 novas features criadas
- ‚úÖ **Dimensionalidade Saud√°vel**: 39 features √© aceit√°vel para XGBoost/Decision Tree
- ‚úÖ **A/B Testing**: F√°cil comparar com/sem Amount posteriormente

**Features Removidas** (1):
- **Time** (cru): Apenas offset temporal no dataset, sem significado em produ√ß√£o

**Features Mantidas** (39):
1. **V1-V28** (28 features): PCA transformations (confidenciais)
2. **Amount Features** (3):
   - Amount (cru) - MANTIDO para testar se ajuda al√©m de Amount_Log
   - Amount_Log - Normaliza distribui√ß√£o assim√©trica
   - Amount_Bin - Categoriza√ß√£o por quartil (0-3)
3. **Time Features** (2):
   - Time_Hour - Hora do dia (0-47h)
   - Time_Period_of_Day - Per√≠odo (0-3: madrugada/manh√£/tarde/noite)
4. **V-Statistics** (5 features):
   - V_Features_Mean, Std, Min, Max, Range
5. **Target** (1): Class (0=leg√≠tima, 1=fraude)

### Fase 2: Modelos Robustos (3 Algoritmos)

**Decis√£o**: Treinar 3 modelos resistentes a outliers (Regress√£o Log√≠stica REMOVIDA)

| Modelo | Vantagens | Desvantagens | Uso |
|--------|-----------|--------------|-----|
| **Decision Tree** | ‚úÖ Splits baseados em ranking<br>‚úÖ Robusto a outliers<br>‚úÖ Interpret√°vel | ‚ùå Overfitting sem poda | ‚úÖ **Baseline** |
| **SVM RBF** | ‚úÖ RobustScaler pre-processing<br>‚úÖ Kernel n√£o-linear<br>‚úÖ Margin-based | ‚ùå Lento para grandes datasets | ‚úÖ **Produ√ß√£o** |
| **XGBoost** | ‚úÖ Ensemble robusto<br>‚úÖ Feature importance nativo<br>‚úÖ Lida com outliers | ‚ùå Hiperpar√¢metros complexos | ‚úÖ **Melhor Performance** |
| ~~Regress√£o Log√≠stica~~ | ‚ùå Assume linearidade<br>‚ùå Sens√≠vel a outliers (11.2%!)<br>‚ùå Assume distribui√ß√£o normal | - | ‚ùå **REMOVIDO** |

**Configura√ß√µes**:
```python
# Decision Tree
DecisionTreeClassifier(
    class_weight='balanced',    # Trata desbalanceamento 1:578
    max_depth=15,               # Evita overfitting
    min_samples_split=50,       # M√≠nimo para split
    random_state=42
)

# SVM RBF
SVC(
    kernel='rbf',               # Kernel n√£o-linear
    class_weight='balanced',    # Trata desbalanceamento
    C=1.0,                      # Regulariza√ß√£o
    gamma='scale',              # Auto-ajuste
    probability=True,           # Para threshold tuning
    random_state=42
)

# XGBoost
XGBClassifier(
    scale_pos_weight=577,       # Ratio 1:578 (desbalanceamento)
    max_depth=6,                # Profundidade √°rvore
    learning_rate=0.1,          # Taxa aprendizado
    n_estimators=100,           # N√∫mero de √°rvores
    subsample=0.8,              # Amostragem para robustez
    colsample_bytree=0.8,       # Feature sampling
    random_state=42
)
```

### Fase 3: Valida√ß√£o Cruzada Estratificada

**M√©todo**: StratifiedKFold (k=5)

**Por qu√™?**:
- ‚úÖ **Mant√©m propor√ß√£o** 0.172% em cada fold
- ‚úÖ **Evita folds sem fraudes** (cr√≠tico com 492 fraudes!)
- ‚úÖ **Valida√ß√£o robusta** contra overfitting
- ‚úÖ **Compara√ß√£o justa** entre modelos

**M√©tricas**:
```python
# Ordem de prioridade (desbalanceamento extremo)
1. PR-AUC (Precision-Recall AUC)       # M√©trica principal
2. Recall (Sensitivity)                # Capturar fraudes > precis√£o
3. F1-Score                            # Balan√ßo geral
4. Precision                           # Evitar alarmes falsos
5. ROC-AUC                             # M√©trica secund√°ria
```

### Fase 4: Grid Search e Threshold Tuning

**Grid Search (PR-AUC scoring)**:
```python
# Decision Tree
param_grid = {
    'max_depth': [10, 15, 20, 25],
    'min_samples_split': [20, 50, 100],
    'min_samples_leaf': [10, 20, 50]
}

# SVM
param_grid = {
    'C': [0.1, 1.0, 10.0],
    'gamma': ['scale', 'auto', 0.001, 0.01]
}

# XGBoost
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.6, 0.8, 1.0]
}
```

**Threshold Tuning**:
```python
# Otimizar threshold (n√£o usar 0.5 padr√£o)
thresholds = np.arange(0.1, 0.9, 0.05)
best_threshold = optimize_for_recall(y_val, y_proba, thresholds)
# Tipicamente: 0.2-0.3 (priorizar Recall)
```

### Fase 5: Feature Importance P√≥s-Treino ‚≠ê

**Objetivo**: Identificar features realmente in√∫teis (n√£o achismo!)

**M√©todo**:
```python
# XGBoost fornece feature importance nativo
importances = xgb_model.feature_importances_
feature_ranking = pd.DataFrame({
    'feature': X_train.columns,
    'importance': importances
}).sort_values('importance', ascending=False)

# Identificar features com importance ‚âà 0
useless_features = feature_ranking[feature_ranking['importance'] < 0.001]['feature'].tolist()
```

**An√°lise Esperada**:
- **Amount vs Amount_Log**: Qual o modelo realmente usa?
- **V13, V15, V22**: Correla√ß√£o baixa, mas √∫til em n√£o-linearidade?
- **V-Statistics**: Agrega√ß√µes ajudam?
- **Time_Hour vs Time_Period_of_Day**: Qual mais discriminativa?

### Fase 6: Refinamento (Se Necess√°rio)

**A/B Testing**:
1. **Treinar com 39 features** (configura√ß√£o atual)
2. **Extrair feature importance** (XGBoost)
3. **Remover features in√∫teis** (importance < 0.001)
4. **Re-treinar modelos**
5. **Comparar m√©tricas**:
   - Se PR-AUC melhora ou mant√©m: ‚úÖ Aceitar remo√ß√£o
   - Se PR-AUC piora: ‚ùå Reverter (features tinham valor!)

**Exemplo**:
```python
# Configura√ß√£o 1: 39 features
model_v1 = train_xgboost(X_39_features, y)
# PR-AUC: 0.85, Recall: 0.82

# Configura√ß√£o 2: Remover Amount (testar se Amount_Log basta)
X_38_features = X_39_features.drop('Amount', axis=1)
model_v2 = train_xgboost(X_38_features, y)
# PR-AUC: 0.86, Recall: 0.83 (MELHOROU!)

# Decis√£o: Atualizar configs.py, incrementar model_version para 1.2.0
```

### Fase 7: Sele√ß√£o do Melhor Modelo

**Crit√©rios de Decis√£o**:
1. **PR-AUC** (peso 50%): M√©trica principal para desbalanceamento
2. **Recall** (peso 30%): Prioridade √© capturar fraudes
3. **F1-Score** (peso 20%): Balan√ßo geral
4. **Tempo de Infer√™ncia**: Considerar em produ√ß√£o
5. **Interpretabilidade**: Decision Tree > XGBoost > SVM

**Compara√ß√£o Estat√≠stica**:
- McNemar's test para comparar erros
- Bootstrapping para confidence intervals
- Cross-validation consistency (variance entre folds)

### Fase 8: Persist√™ncia e Versionamento

**Salvamento**:
```python
# Melhor modelo
joblib.dump(best_model, 'models/best_model.pkl')

# Todos os modelos (compara√ß√£o)
joblib.dump(tree_model, 'models/tree_model_v1.1.0.pkl')
joblib.dump(svm_model, 'models/svm_model_v1.1.0.pkl')
joblib.dump(xgb_model, 'models/xgboost_model_v1.1.0.pkl')

# Scalers (j√° salvos)
# models/scalers_v1.1.0.pkl
```

**Metadados no PostgreSQL**:
```sql
INSERT INTO trained_models (
    model_name, algorithm, version, feature_count,
    pr_auc, recall, f1_score, threshold, is_active
) VALUES (
    'xgboost_v1.1.0', 'XGBoost', '1.1.0', 39,
    0.86, 0.83, 0.78, 0.25, TRUE
);
```

---

## üìã 10. Roadmap de Implementa√ß√£o

### ‚úÖ Etapa 1-3: Conclu√≠das
- EDA completa
- Outlier analysis (preservar todos)
- Missing values (zero encontrados)
- Feature engineering (9 novas features)
- Feature selection pragm√°tica (Time removido)

### ‚è≥ Etapa 4: Treinamento de Modelos (PR√ìXIMO)
**Scripts a criar**:
1. `training/01_train_models.py` - Treinamento dos 3 modelos
2. `training/02_hyperparameter_tuning.py` - Grid Search com PR-AUC
3. `training/03_threshold_tuning.py` - Otimiza√ß√£o de threshold
4. `training/04_feature_importance.py` - An√°lise p√≥s-treino

**Outputs esperados**:
- `models/tree_model_v1.1.0.pkl`
- `models/svm_model_v1.1.0.pkl`
- `models/xgboost_model_v1.1.0.pkl`
- `reports/model_comparison.json`
- `reports/feature_importance.json`

### ‚è≥ Etapa 5: Refinamento (Se Necess√°rio)
- A/B testing (39 features vs subset)
- Remo√ß√£o de features in√∫teis (baseado em importance)
- Re-treinamento e compara√ß√£o
- Decis√£o final versionada em configs.py

### ‚è≥ Etapa 6: Dashboard Flask (MVP)
- Interface interativa
- Upload de transa√ß√£o
- Classifica√ß√£o em tempo real
- Visualiza√ß√£o de m√©tricas

---

## üéì Aprendizados Chave

1. **Outliers ‚â† Erros**: 18.5% das fraudes em outliers (cr√≠tico manter!)
2. **Correla√ß√£o Linear ‚â† Utilidade**: XGBoost captura n√£o-linearidades
3. **Feature Importance P√≥s-Treino**: Deixar modelo decidir > intui√ß√£o humana
4. **Pragmatismo > Perfei√ß√£o**: 39 features √© saud√°vel, come√ßar simples
5. **A/B Testing**: Git versiona decis√µes, f√°cil comparar configura√ß√µes
6. **Threshold Tuning**: 0.2-0.3 melhor que 0.5 para fraudes raras
7. **RobustScaler**: Resistente a outliers (mediana/IQR)
8. **StratifiedKFold**: Essencial para desbalanceamento extremo
9. **PR-AUC > ROC-AUC**: M√©trica certa para classes desbalanceadas
10. **Modelos Robustos**: Decision Tree, SVM RBF, XGBoost (Regress√£o Log√≠stica removida)

**Scalers (Pickle + Versionamento)**:
```python
# MVP/POC (atual)
models/scalers.pkl  # Vers√£o ativa

# Produ√ß√£o (recomendado)
models/scalers_v1.0.0.pkl  # Sem feature selection
models/scalers_v1.1.0.pkl  # Com V13, V15, V22 removidos

# Carregar vers√£o espec√≠fica
from src.ml.models.configs import config
scaler_path = config.paths.get_versioned_scaler_path('1.1.0')
scalers = joblib.load(scaler_path)
```

**Por que Pickle?**
- ‚úÖ Preserva objeto completo (m√©todos + estado)
- ‚úÖ R√°pido (serializa√ß√£o nativa Python)
- ‚úÖ Integra√ß√£o perfeita com scikit-learn
- ‚úÖ Tamanho pequeno (~100 bytes para RobustScaler)

**Alternativas para Produ√ß√£o**:
- **MLflow**: Versionamento autom√°tico + tracking completo
- **ONNX**: Cross-platform (C++, Java, etc.)
- **Redis**: Cache distribu√≠do para alta disponibilidade

---

## üéì Storytelling para Apresenta√ß√£o

### Decis√µes T√©cnicas Principais

1. **Desbalanceamento**: 
   > "Com apenas 492 fraudes em 284.807 transa√ß√µes (0.172%), optamos por N√ÉO usar SMOTE ou undersampling. Em vez disso, aplicamos `class_weight='balanced'` nos modelos e threshold tuning p√≥s-treino para otimizar recall."

2. **Outliers**: 
   > "An√°lise IQR identificou 31.904 outliers (11.2%), mas 91 fraudes (18.5%) est√£o nesta faixa. Decidimos manter TODOS os dados, pois cada fraude √© preciosa. Solu√ß√£o: RobustScaler para normaliza√ß√£o resistente a outliers."

3. **Sele√ß√£o de Modelos (3 Modelos Robustos)**:
   > "Removemos Regress√£o Log√≠stica pois assume linearidade e √© sens√≠vel a outliers. Mantivemos apenas modelos robustos: **Decision Tree** (splits por ranking, n√£o afetado por valores extremos), **SVM com RBF kernel** (normalizado com RobustScaler, captura n√£o-linearidades), e **XGBoost** (√°rvores + regulariza√ß√£o, extremamente robusto). Todos lidam bem com outliers e desbalanceamento via `class_weight='balanced'` e `scale_pos_weight`."

4. **Features com Baixa Correla√ß√£o**: 
   > "Amount e Time t√™m correla√ß√£o linear quase zero com fraude, mas an√°lise revelou padr√µes n√£o-lineares: 73.6% das fraudes < $100 e taxa de fraude 10x maior √† madrugada. Feature engineering (Time_Period, Amount_Bin, estat√≠sticas V-features) e modelos n√£o-lineares (XGBoost) capturam essas rela√ß√µes complexas."

5. **Otimiza√ß√£o de Performance**:
   > "Implementamos PostgreSQL COPY para bulk inserts (81.6% mais r√°pido) e paraleliza√ß√£o de steps independentes (ThreadPoolExecutor). Pipeline completo passou de ~130s para ~62s, um ganho de 52%. Metadata tracking evita duplica√ß√£o de dados quando n√£o h√° altera√ß√£o."

6. **Feature Engineering Estrat√©gico**:
   > "Criamos 9 novas features baseadas na EDA: Time_Period captura padr√£o temporal (fraudes 10x maiores √† madrugada), Amount_Bin categoriza valores (73.6% fraudes < $100), estat√≠sticas V-features capturam tend√™ncia central e variabilidade. Total: 31 ‚Üí 40 features sem explos√£o combinat√≥ria."

7. **Feature Selection Config-Driven**:
   > "Implementamos an√°lise automatizada (Step 05.5) que gera relat√≥rio JSON com Pearson, Spearman, Mutual Information, VIF e redund√¢ncia. Desenvolvedor revisa relat√≥rio, considera contexto de neg√≥cio, e atualiza `configs.py` manualmente. Step 06 aplica remo√ß√£o automaticamente. Abordagem rastre√°vel (Git) e reprodut√≠vel, permitindo A/B testing f√°cil."

---

## üìö Refer√™ncias T√©cnicas

- **IQR (Tukey's Method)**: Identifica√ß√£o cient√≠fica de outliers
- **Pearson Correlation**: Rela√ß√µes lineares
- **Spearman Correlation**: Rela√ß√µes monot√¥nicas
- **Mutual Information**: Rela√ß√µes n√£o-lineares gerais
- **RobustScaler**: Normaliza√ß√£o resistente a outliers (usa mediana e IQR)
- **class_weight='balanced'**: Ajuste autom√°tico de pesos por classe
- **Threshold Tuning**: Otimiza√ß√£o de ponto de decis√£o (Precision-Recall trade-off)
- **PostgreSQL COPY**: Bulk insert nativo (70-80% mais r√°pido que to_sql)
- **ThreadPoolExecutor**: Paraleliza√ß√£o de tarefas I/O-bound

---

**Conclus√£o**: EDA revelou que o dataset √© extremamente desbalanceado (0.172% fraudes), tem outliers importantes (18.5% das fraudes), e features com baixa correla√ß√£o linear mas alta utilidade n√£o-linear. Decis√µes foram baseadas em an√°lise estat√≠stica rigorosa e contexto do problema (detec√ß√£o de fraude = recall cr√≠tico). Pipeline otimizado processa 284k transa√ß√µes em ~62s com rastreabilidade completa em PostgreSQL. Feature selection implementada com an√°lise automatizada (Step 05.5) e decis√£o manual config-driven (Step 06), permitindo versionamento e A/B testing.

---

**Autor**: Fraud Detection Team  
**Data**: 2025-10-02  
**Vers√£o**: 4.0.0 (Pipeline otimizado + Steps 04-07 + Feature Selection)
