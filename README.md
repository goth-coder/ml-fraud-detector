# ğŸš¨ DetecÃ§Ã£o de Fraude em CartÃ£o de CrÃ©dito
## Tech Challenge Fase 3 - FIAP

Sistema completo de **Machine Learning para detecÃ§Ã£o de fraude** com pipeline modular, PostgreSQL para persistÃªncia, CLI interativo e **Dashboard Web Flask**.

---

## ğŸ¯ Objetivo

Desenvolver sistema de detecÃ§Ã£o de fraude usando **XGBoost otimizado** com:
- âœ… Tratamento de dados **altamente desbalanceados** (ratio 1:578)
- âœ… Pipeline modular MVC-ML (processamento â†’ treinamento â†’ inferÃªncia)
- âœ… ValidaÃ§Ã£o robusta com **StratifiedKFold**
- âœ… **Grid Search automatizado** com versionamento de hiperparÃ¢metros
- âœ… **PostgreSQL** (dados) + **Pickle** (modelos) para rastreabilidade
- âœ… **CLI completo** (pipeline/train/tune/predict) com 4 modos de operaÃ§Ã£o
- âœ… **Backend Flask REST API** com simulaÃ§Ã£o de transaÃ§Ãµes em tempo real
---

## ğŸ—ï¸ Arquitetura do Projeto

### Estrutura MVC-ML Modular
 
```
ml-fraud-detector/
â”œâ”€â”€ main.py                      # ğŸ¯ CLI principal (pipeline/train/tune/predict)
â”œâ”€â”€ pyproject.toml               # ConfiguraÃ§Ã£o do projeto (uv/pip)
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ README.md                    # Este arquivo
â”‚
â”œâ”€â”€ data/                        # ğŸ“Š Datasets e configuraÃ§Ãµes
â”‚   â”œâ”€â”€ creditcard.csv           # Dataset original (284,807 transaÃ§Ãµes)
â”‚   â”œâ”€â”€ xgboost_hyperparameters.json  # HiperparÃ¢metros ativos
â”‚   â”œâ”€â”€ archive/                 # VersÃµes antigas de hiperparÃ¢metros
â”‚   â””â”€â”€ examples/                # Exemplos de transaÃ§Ãµes para teste
â”‚       â”œâ”€â”€ fraud_transaction.json
â”‚       â””â”€â”€ legitimate_transaction.json
â”‚
â”œâ”€â”€ models/                      # ğŸ¤– Modelos treinados
â”‚   â”œâ”€â”€ scalers.pkl              # RobustScaler + StandardScaler
â”‚   â”œâ”€â”€ xgboost_v2.1.0.pkl       # â­ Modelo em produÃ§Ã£o
â”‚   â””â”€â”€ archive/                 # VersÃµes antigas de modelos
â”‚
â”œâ”€â”€ reports/                     # ğŸ“ˆ RelatÃ³rios gerados (JSON)
â”‚   â””â”€â”€ feature_selection_report.json
â”‚
â”œâ”€â”€ database/                    # ğŸ—„ï¸ ConfiguraÃ§Ã£o PostgreSQL
â”‚   â”œâ”€â”€ docker-compose.yml       # Docker setup (PostgreSQL 15)
â”‚   â””â”€â”€ schema.sql               # Schema completo (7 tabelas pipeline + 3 metadata)
â”‚
â”œâ”€â”€ docs/                        # ğŸ“š DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ EDA_REPORT.md            # AnÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ MODEL_SELECTION.md       # ComparaÃ§Ã£o de 4 modelos
â”‚   â”œâ”€â”€ DATA_ARCHITECTURE.md     # Arquitetura PostgreSQL + Pickle
â”‚   â”œâ”€â”€ DECISOES_TECNICAS.md     # DecisÃµes ML + otimizaÃ§Ãµes
â”‚   â”œâ”€â”€ TRANSACTION_EXAMPLES.md  # Exemplos prÃ¡ticos de transaÃ§Ãµes (fraude vs legÃ­tima)
â”‚   â””â”€â”€ images/                  # GrÃ¡ficos EDA
â”‚       â”œâ”€â”€ 01_class_distribution.png
â”‚       â”œâ”€â”€ 02_amount_analysis.png
â”‚       â”œâ”€â”€ 03_correlation_heatmap.png
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ src/                         # ğŸ’» CÃ³digo fonte
    â”œâ”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ ml/                      # ğŸ§  Machine Learning
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ README.md            # DocumentaÃ§Ã£o da arquitetura MVC-ML
    â”‚   â”‚
    â”‚   â”œâ”€â”€ processing/          # ğŸ”§ FunÃ§Ãµes de processamento
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ loader.py        # PostgreSQL COPY (otimizado 81.6%)
    â”‚   â”‚   â”œâ”€â”€ validation.py    # Schema e integridade
    â”‚   â”‚   â”œâ”€â”€ cleaning.py      # AnÃ¡lise de outliers
    â”‚   â”‚   â”œâ”€â”€ normalization.py # RobustScaler + StandardScaler
    â”‚   â”‚   â”œâ”€â”€ feature_engineering.py  # 9 novas features
    â”‚   â”‚   â”œâ”€â”€ feature_selection.py    # AnÃ¡lise automatizada
    â”‚   â”‚   â”œâ”€â”€ splitters.py     # Stratified train/test split
    â”‚   â”‚   â””â”€â”€ metadata.py      # Pipeline metadata tracking
    â”‚   â”‚
    â”‚   â”œâ”€â”€ pipelines/           # ğŸš€ OrquestraÃ§Ã£o
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ data_pipeline.py # Steps 01-07 (completo)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ training/            # ğŸ“ Treinamento ML
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ train.py         # Treino com JSON configs
    â”‚   â”‚   â””â”€â”€ tune.py          # Grid Search + auto-update JSON
    â”‚   â”‚
    â”‚   â””â”€â”€ models/              # âš™ï¸ ConfiguraÃ§Ãµes
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ configs.py       # Dataclasses + carregamento JSON
    â”‚
    â”œâ”€â”€ api/                     # ğŸŒ Backend Flask
    â”‚   â”œâ”€â”€ __init__.py          # Factory pattern (create_app)
    â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes (Dev/Prod/Test)
    â”‚   â””â”€â”€ routes.py            # Endpoints REST
    â”‚
    â”œâ”€â”€ models/                  # ğŸ“¦ Data models
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ database_models.py   # SQLAlchemy models (pipeline + webapp)
    â”‚
    â””â”€â”€ services/                # ğŸ”Œ ServiÃ§os
        â”œâ”€â”€ ml/                  # ğŸ¤– ML Services
        â”‚   â”œâ”€â”€ model_service.py      # Singleton para inferÃªncia
        â”‚   â””â”€â”€ transaction_generator.py  # Gerador de transaÃ§Ãµes
        â”‚
        â””â”€â”€ database/            # ğŸ—„ï¸ ConexÃ£o PostgreSQL
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ connection.py    # Engine + connection pooling
            â””â”€â”€ database_service.py  # CRUD para webapp
```
 

### Pipeline de Dados (7 Steps)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PROCESSING PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  CSV Raw (284,807) â†’ [01] Load Raw â†’ PostgreSQL (raw_transactions)   â”‚
â”‚                         â†“                                            â”‚
â”‚                    [02] Outlier Analysis â†’ Metadata only             â”‚
â”‚                         â†“                                            â”‚
â”‚                    [03] Missing Values â†’ Metadata only               â”‚
â”‚                         â†“                                            â”‚
â”‚                    [04] Normalize â†’ PostgreSQL + scalers.pkl         â”‚
â”‚                         â†“                                            â”‚
â”‚                    [05] Feature Engineering â†’ PostgreSQL (40 cols)   â”‚
â”‚                         â†“ (Time_Period, Amount_Log, stats)           â”‚
â”‚                    [05.5] Feature Selection Analysis â†’ JSON Report   â”‚
â”‚                         â†“ (Pearson, Spearman, MI, VIF)               â”‚
â”‚                    [06] Apply Feature Selection â†’ PostgreSQL (33)    â”‚
â”‚                         â†“ (Config-driven removal)                    â”‚
â”‚                    [07] Train/Test Split â†’ train_data + test_data    â”‚
â”‚                         â†“ (StratifiedKFold 80/20)                    â”‚
â”‚                                                                      â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitetura de PersistÃªncia

**PostgreSQL - Tabelas do Pipeline**:
- `raw_transactions` (284,807 linhas): CSV original
- `cleaned_transactions` (284,804 linhas): Sem outliers
- `imputed_transactions` (284,804 linhas): Sem missing values
- `normalized_transactions` (284,804 linhas): Features normalizadas
- `engineered_transactions` (284,804 linhas): AdiÃ§Ã£o de features novas 
- `train_features` (~227,843 linhas): 80% treino
- `test_features` (~56,961 linhas): 20% teste

**PostgreSQL - Tabelas do Webapp** (âœ… **IMPLEMENTADAS**):
- `classification_results`: HistÃ³rico de prediÃ§Ãµes do dashboard
  - `model_version`, `predicted_at`, `is_fraud`, `fraud_probability`
  - `transaction_features` (JSONB com 33 features)
  - Ãndices: `predicted_at`, `is_fraud`, `model_version`
- `simulated_transactions`: TransaÃ§Ãµes geradas pelo simulador
  - `transaction_type` ('legitimate', 'fraud')
  - `features` (JSONB), `classification_id` (FK)

**PostgreSQL - Tabelas de Metadados** (ğŸ”® FUTURO - nÃ£o implementadas):
- `metrics_history`: MÃ©tricas de modelos ao longo do tempo
- `trained_models`: Registro de modelos treinados com timestamps
- `data_splits`: HistÃ³rico de splits train/test

**Pickle (modelos ML)**:
- `models/scalers.pkl`: RobustScaler + StandardScaler (fit em train)
- `models/xgboost_v2.1.0.pkl`: Modelo final em produÃ§Ã£o â­

**JSON (hiperparÃ¢metros versionados)**:
- `data/xgboost_hyperparameters.json`: HiperparÃ¢metros ativos
- `data/archive/`: VersÃµes anteriores com timestamp automÃ¡tico

**Por que hÃ­brido?**
- **PostgreSQL**: Rastreabilidade, SQL analytics, backups automÃ¡ticos, histÃ³rico webapp
- **Pickle**: Fast loading (~0.1s), mÃ©todos do objeto preservados, scikit-learn nativo
- **JSON**: Versionamento de hiperparÃ¢metros, comparaÃ§Ãµes dinÃ¢micas, production-ready

### Stack TecnolÃ³gico
- **Data Pipeline**: Python 3.13 + Pandas + PostgreSQL 15
- **ML Model**: XGBoost 3.0.5 (scale_pos_weight=577, max_depth=6)
- **Backend API**: Flask 3.0.3 + SQLAlchemy (REST endpoints)
- **PersistÃªncia**: PostgreSQL (dados + histÃ³rico webapp) + Pickle (modelos)
- **OtimizaÃ§Ã£o**: PostgreSQL COPY, ThreadPoolExecutor, metadata-only steps
- **Infraestrutura**: Docker Compose (PostgreSQL)

---

## ğŸ“Š Dataset

- **Fonte**: Kaggle - Credit Card Fraud Detection
- **TransaÃ§Ãµes**: 284.807 (2 dias, setembro 2013)
- **Features**: 30 (Time, Amount, V1-V28 via PCA)
- **Target**: Class (0=legÃ­tima, 1=fraude)
- **Desbalanceamento**: 492 fraudes (0.172%) vs 284.315 legÃ­timas

---

## ğŸš€ Quick Start

### 1. PrÃ©-requisitos
- Python 3.13+
- PostgreSQL 15+ (ou Docker)
- 2GB+ RAM disponÃ­vel

### 2. InstalaÃ§Ã£o

```bash
# Clone do repositÃ³rio
git clone <repo-url>
cd ml-fraud-detector

# Criar ambiente virtual
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# OU usando uv (mais rÃ¡pido)
uv pip install -e .
```

### 3. Configurar PostgreSQL

**OpÃ§Ã£o A: Docker (Recomendado)**
```bash
cd database
docker-compose up -d

# Validar conexÃ£o
docker exec -it fraud_detection_postgres psql -U fraud_user -d fraud_detection -c "\dt"
```

**OpÃ§Ã£o B: PostgreSQL Local**
```bash
# Criar banco e usuÃ¡rio
createdb fraud_detection
createuser fraud_user --password fraud_pass_dev

# Aplicar schema
psql -U fraud_user -d fraud_detection -f database/schema.sql
```

### 4. Executar Pipeline de Dados

```bash
# OpÃ§Ã£o 1: Via CLI (RECOMENDADO)
python main.py pipeline

# OpÃ§Ã£o 2: Direto (legacy)
python -m src.ml.pipelines.data_pipeline

# Resultado esperado (~62s):
# âœ… raw_transactions: 284,807 linhas
# âœ… cleaned_transactions: 284,804 linhas (outliers removidos)
# âœ… imputed_transactions: 284,804 linhas (missing tratados)
# âœ… normalized_transactions: 284,804 linhas (RobustScaler aplicado)
# âœ… engineered_transactions: 284,804 linhas (40 features)
# âœ… train_features: ~227,843 linhas (394 fraudes)
# âœ… test_features: ~56,961 linhas (98 fraudes)
# âœ… scalers.pkl salvo em models/
```

---

## ğŸ–¥ï¸ CLI Principal (`main.py`)

O projeto inclui um **CLI completo** com 4 modos de operaÃ§Ã£o:

### Modo 1: Pipeline (Processamento de Dados)

Executa pipeline completo de dados (Steps 01-07, ~62s):

```bash
# Pipeline completo
python main.py pipeline

# SaÃ­da esperada: 
# âœ… Pipeline completo!
# ğŸ“Š Resultado esperado:
#    âœ… train_features: ~227,843 linhas (394 fraudes)
#    âœ… test_features: ~56,961 linhas (98 fraudes)
#    âœ… scalers.pkl salvo em models/
# 
# ğŸ’¡ PrÃ³ximo passo: python main.py train
```

**Quando usar**: Primeira vez ou apÃ³s mudanÃ§as no dataset original.
 


### Modo 2: Train (Treinamento RÃ¡pido)

Treina modelo usando hiperparÃ¢metros de `data/xgboost_hyperparameters.json` (~1 min):

```bash
# Treinar XGBoost com JSON ativo (assume dados jÃ¡ no PostgreSQL)
python main.py train

# Especificar versÃ£o personalizada
python main.py train --model-version 2.1.1

# Overwrite modelo existente (move antigo para models/archive/)
python main.py train --model-version 2.1.0 -ow

# SaÃ­da esperada:
# ğŸš€ MODO: TREINAMENTO
# âœ… Train: 227,845 linhas (394 fraudes)
# âœ… Test: 56,962 linhas (98 fraudes)
# ğŸ“Š Cross-Validation (StratifiedKFold k=5)...
#    Precision (CV): 0.8123 Â± 0.0234
#    Recall (CV): 0.8456 Â± 0.0189
# âœ… Modelo salvo: models/xgboost_v2.1.0.pkl
#    PR-AUC: 0.8772
#    Precision: 86.60%
#    False Positives: 13
```

**Versionamento AutomÃ¡tico:**
- **Sem `-ow`**: Salva em `models/archive/xgboost_v{version}_{timestamp}.pkl`
- **Com `-ow`**: Move modelo antigo para archive e cria novo em `models/`

**Quando usar**: Treinar rapidamente com parÃ¢metros conhecidos bons (JSON ativo).

---

### Modo 3: Tune (Grid Search AutomÃ¡tico)

Executa **Grid Search** (~20 min) e **atualiza `data/xgboost_hyperparameters.json`** automaticamente:

```bash
# Grid Search (modo archive - padrÃ£o)
python main.py tune

# Grid Search + overwrite do JSON ativo
python main.py tune -ow

# Especificar versÃ£o otimizada
python main.py tune --model-version 3.0.0 -ow

# SaÃ­da esperada:
# ğŸ”§ Executando Grid Search...
# âœ… Grid Search completo!
# ğŸ† Melhores ParÃ¢metros:
#    colsample_bytree: 0.7
#    learning_rate: 0.3
#    max_depth: 6
#    ...
#    PR-AUC: 0.8772
#    Atualizando configs.py com melhores hiperparÃ¢metros...
#    âœ… configs.py atualizado!
```

**Quando usar**: 
- Otimizar hiperparÃ¢metros apÃ³s mudanÃ§as no dataset (novas features, etc.)
- Experimentar novos grids de busca

**SeguranÃ§a**:
- Cria **backup automÃ¡tico** de `configs.py` antes de atualizar
- Modo `--auto-update` para pipelines CI/CD
- Modo interativo pede confirmaÃ§Ã£o antes de atualizar

---

### Modo 4: Predict (InferÃªncia)

Executa prediÃ§Ãµes em **CSV** ou **JSON**:

**OpÃ§Ã£o A: CSV**
```bash
python main.py predict --file data/new_transactions.csv

# SaÃ­da esperada:
# ğŸ”® MODO: PREDIÃ‡ÃƒO (CSV)
# ğŸ“¥ Carregando modelo: xgboost_v2.1.0.pkl
# âœ… 1,000 transaÃ§Ãµes carregadas
# âœ… PrediÃ§Ãµes completas!
# ğŸ“Š Resumo:
#    Total transaÃ§Ãµes: 1,000
#    Fraudes detectadas: 3 (0.30%)
#    LegÃ­timas: 997
# ğŸ’¾ Resultados salvos: data/new_transactions_predictions.csv
#
# âš ï¸  Top 5 Fraudes com Maior Probabilidade:
#    Linha 42: 95.23% probabilidade
#    Linha 157: 89.45% probabilidade
#    Linha 891: 78.12% probabilidade
```

**OpÃ§Ã£o B: JSON (arquivo)**
```bash
python main.py predict --json-file input.json

# input.json exemplo:
# [
#   {"V1": -0.5, "V2": 1.2, "V3": 0.8, ..., "Amount_Log": 4.5},
#   {"V1": 0.3, "V2": -0.9, "V3": 1.1, ..., "Amount_Log": 3.2}
# ]

# SaÃ­da:
# ğŸ”® MODO: PREDIÃ‡ÃƒO (JSON)
# âœ… 2 transaÃ§Ãµes carregadas
# ğŸ“‹ Resultados:
#    ğŸš¨ TransaÃ§Ã£o 0: FRAUDE (95.23%)
#    âœ… TransaÃ§Ã£o 1: LEGÃTIMA (2.45%)
# ğŸ’¾ Resultados salvos: prediction_results.json
```

**OpÃ§Ã£o C: JSON (inline)**
```bash
python main.py predict --json '{"V1": -0.5, "V2": 1.2, "V3": 0.8, "Amount_Log": 4.5}'

# SaÃ­da:
# ğŸ”® MODO: PREDIÃ‡ÃƒO (JSON)
# âœ… 1 transaÃ§Ã£o carregada
# ğŸ“‹ Resultados:
#    ğŸš¨ TransaÃ§Ã£o 0: FRAUDE (89.34%)
```

**Quando usar**:
- **CSV**: Batch processing de muitas transaÃ§Ãµes
- **JSON file**: IntegraÃ§Ã£o com APIs/sistemas externos
- **JSON inline**: Testes rÃ¡pidos, CI/CD pipelines

---

## ğŸŒ Backend Flask API

O projeto inclui um **backend REST API completo** para dashboard de detecÃ§Ã£o de fraudes em tempo real.

### Arquitetura do Backend

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (HTML + JS) [FUTURO]           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ BotÃ£o    â”‚  â”‚ BotÃ£o    â”‚  â”‚ Stats    â”‚     â”‚
â”‚  â”‚ LegÃ­tima â”‚  â”‚ Fraude   â”‚  â”‚ Dashboardâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Flask API REST (src/api/)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ POST /api/simulate (gera + classifica)  â”‚   â”‚
â”‚  â”‚ GET  /api/stats (estatÃ­sticas 24h)      â”‚   â”‚
â”‚  â”‚ GET  /api/history (Ãºltimas 50)          â”‚   â”‚
â”‚  â”‚ GET  /health (health check)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ML Services (Singleton)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ TransactionGenerator                   â”‚    â”‚
â”‚  â”‚  â†’ Busca transaÃ§Ãµes reais do test_dataâ”‚    â”‚
â”‚  â”‚  â†’ 56,864 legÃ­timas + 98 fraudes       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                   â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ModelService (XGBoost v2.1.0)          â”‚    â”‚
â”‚  â”‚  â†’ PrediÃ§Ã£o <100ms                     â”‚    â”‚
â”‚  â”‚  â†’ 33 features validadas               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                   â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ DatabaseService                        â”‚    â”‚
â”‚  â”‚  â†’ save_classification()               â”‚    â”‚
â”‚  â”‚  â†’ save_transaction()                  â”‚    â”‚
â”‚  â”‚  â†’ get_history() / get_stats()         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PostgreSQL 15                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ classification_results (histÃ³rico)      â”‚   â”‚
â”‚  â”‚ simulated_transactions (transaÃ§Ãµes)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Endpoints DisponÃ­veis

#### 1. **POST /api/simulate** - Simular TransaÃ§Ã£o
```bash
curl -X POST http://localhost:5000/api/simulate \
  -H "Content-Type: application/json" \
  -d '{"transaction_type": "legitimate"}'

# Response:
{
  "success": true,
  "transaction_id": 123,
  "classification_id": 456,
  "is_fraud": false,
  "fraud_probability": 0.0023,
  "fraud_probability_percent": "0.23%",
  "confidence": "Baixa",
  "model_version": "v2.1.0",
  "predicted_at": "2025-10-06T10:30:00",
  "transaction_type": "legitimate"
}
```

#### 2. **GET /api/stats** - EstatÃ­sticas Agregadas
```bash
curl http://localhost:5000/api/stats?hours=24

# Response:
{
  "success": true,
  "stats": {
    "total": 150,
    "fraud_count": 30,
    "fraud_percentage": 20.0,
    "avg_probability": 0.4523,
    "by_hour": [...]
  }
}
```

#### 3. **GET /api/history** - HistÃ³rico de ClassificaÃ§Ãµes
```bash
curl http://localhost:5000/api/history?limit=10

# Response:
{
  "success": true,
  "count": 10,
  "history": [
    {
      "id": 456,
      "predicted_at": "2025-10-06T10:30:00",
      "is_fraud": true,
      "fraud_probability": 0.9876,
      "model_version": "v2.1.0"
    },
    ...
  ]
}
```

#### 4. **GET /health** - Health Check
```bash
curl http://localhost:5000/health

# Response:
{
  "status": "healthy",
  "model_version": "v2.1.0"
}
```

### Iniciar o Servidor Flask

```bash
# Ativar ambiente virtual
source .venv/bin/activate  # Linux/Mac

# Instalar dependÃªncias do webapp
pip install flask flask-cors

# Iniciar servidor de desenvolvimento
python run.py

# Servidor rodando em:
# http://127.0.0.1:5000
```


ğŸ“š **DocumentaÃ§Ã£o Completa da API**: [`docs/API_ENDPOINTS.md`](docs/API_ENDPOINTS.md)

---

## ğŸ“š DocumentaÃ§Ã£o Completa

### Planos EstratÃ©gicos
- **`docs/plan_main.md`**: Plano MVP completo com XGBoost v1.1.0, v2.0.0, v2.1.0
- **`docs/plan_kafka.md`**: Enhancement Kafka (opcional, nÃ£o implementado)
- **`docs/changelog.md`**: Registro de todas as mudanÃ§as

### RelatÃ³rios TÃ©cnicos
- **`docs/EDA_REPORT.md`**: AnÃ¡lise exploratÃ³ria de dados completa
- **`docs/MODEL_SELECTION.md`**: ComparaÃ§Ã£o de 4 modelos + 3 versÃµes XGBoost
- **`docs/DATA_ARCHITECTURE.md`**: Arquitetura PostgreSQL + Pickle
- **`docs/API_ENDPOINTS.md`**: DocumentaÃ§Ã£o completa dos endpoints REST Flask
 
### Arquitetura MVC-ML
- **`src/ml/README.md`**: DocumentaÃ§Ã£o completa da arquitetura modular
 
    
---

## ğŸ”§ ConfiguraÃ§Ãµes Importantes

### `src/ml/models/configs.py`

```python
# HiperparÃ¢metros XGBoost v2.1.0 (Production)
xgboost_params = {
    'colsample_bytree': 0.7,      # 70% features por Ã¡rvore
    'learning_rate': 0.3,          # Taxa de aprendizado alta
    'max_depth': 6,                # Ãrvores profundas (nÃ£o-linear)
    'min_child_weight': 1,         # MÃ­nimo peso folha
    'n_estimators': 100,           # 100 Ã¡rvores
    'scale_pos_weight': 577,       # Compensa desbalanceamento
    'subsample': 0.7,              # 70% amostras por Ã¡rvore
    'eval_metric': 'aucpr'         # PR-AUC como mÃ©trica
}

# Features Removidas (Config-Driven)
excluded_features = [
    'Time',              # Offset temporal sem sentido real
    'Time_Period_of_Day', # Derivada de Time
    'V8', 'V23',         # Baixa correlaÃ§Ã£o (<0.01)
    'Amount',            # Amount_Log Ã© melhor
    'V13'                # Redundante (alta correlaÃ§Ã£o com V17)
]
```

---

## ğŸ“Š MÃ©tricas de Performance

### Pipeline de Tratamento de Dados (52% mais rÃ¡pido âš¡)

| Step | Antes | Depois | Ganho | OtimizaÃ§Ã£o |
|------|-------|--------|-------|------------|
| **04 Normalize** | 90s | 16.5s | **81.6%** ğŸ”¥ | PostgreSQL COPY |
| **02+03 Parallel** | 4.8s | 3.9s | **18.6%** âš¡ | ThreadPoolExecutor |
| **Pipeline Total** | ~130s | **~62s** | **52%** ğŸš€ | Multi-otimizaÃ§Ã£o |

### Modelos XGBoost (Test Set)

| MÃ©trica | v1.1.0 | v2.0.0 | v2.1.0 â­ |
|---------|--------|--------|----------|
| **PR-AUC** | 0.8719 | 0.8847 | 0.8772 |
| **ROC-AUC** | 0.9738 | 0.9801 | 0.9765 |
| **Precision** | 72.27% | 85.42% | **86.60%** |
| **Recall** | 87.76% | 83.67% | 81.63% |
| **F1-Score** | 79.26% | 84.54% | 84.04% |
| **True Positives** | 86 | 82 | 80 |
| **False Positives** | 33 | 14 | **13** |
| **False Negatives** | 12 | 16 | 18 |
| **True Negatives** | 56,831 | 56,850 | 56,851 |

--- 
 
## Para saber mais ğŸ§ 
---

## ğŸ”„ Pipeline de Tratamento de Dados (Modular)

Cada step Ã© **isolado e reutilizÃ¡vel**, podendo ser orquestrado como **DAG no Airflow**:

### âœ… Step 01: Load Raw Data
- **Input**: `data/creditcard.csv`
- **Output**: PostgreSQL `raw_transactions` (284,807 linhas)
- **FunÃ§Ã£o**: Carregar e validar schema

### âœ… Step 02: Outlier Analysis
- **Input**: `raw_transactions`
- **Output**: `cleaned_transactions` (284,807 linhas - sem remoÃ§Ã£o)
- **FunÃ§Ã£o**: AnÃ¡lise de outliers (IQR, Z-score)
- **Justificativa**: Outliers preservados, visto a natureza do dataset desbalanceado

### âœ… Step 03: Handle Missing Values
- **Input**: `cleaned_transactions`
- **Output**: `imputed_transactions`
- **FunÃ§Ã£o**: ImputaÃ§Ã£o (mediana) ou remoÃ§Ã£o (Class)

### âœ… Step 04: Normalize Features 
- **Input**: `imputed_transactions`
- **Output**: `normalized_transactions` + `models/scalers.pkl`
- **FunÃ§Ã£o**: RobustScaler (Amount), StandardScaler (Time)

### âœ… Step 05: Feature Engineering 
- **Input**: `normalized_transactions`
- **Output**: `engineered_transactions`
- **FunÃ§Ã£o**: Time_Period, Amount_Bin, Amount_Log, V_Interactions

### âœ… Step 06: Train/Test Split
- **Input**: `engineered_transactions`
- **Output**: `train_features`, `test_features`, `train_target`, `test_target`
- **FunÃ§Ã£o**: StratifiedShuffleSplit (80/20)

ğŸ“š **DocumentaÃ§Ã£o completa**: [`src/ml/README.md`](src/ml/README.md)

---

## ğŸ“Š AnÃ¡lise ExploratÃ³ria (EDA)
### Insights Principais

**Desbalanceamento**:
- Fraudes: 492 (0.172%)
- LegÃ­timas: 284.315 (99.828%)
- Ratio: 1:578

**Amount (Valores)**:
- Fraudes: Mediana $9.25, MÃ©dia $122.21
- 73.6% das fraudes tÃªm Amount < $100
- Nenhuma fraude tem Amount > $15k

**CorrelaÃ§Ãµes (Top 3)**:
- V17: -0.326 (mais forte!)
- V14: -0.302
- V12: -0.261

**Temporal**:
- Fraudes concentradas em certos horÃ¡rios
- Taxa de fraude varia 10x ao longo do dia

ğŸ“Š **GrÃ¡ficos**: `docs/images/*.png`  
ğŸ“‹ **RelatÃ³rio Completo**: [`docs/EDA_REPORT.md`](docs/EDA_REPORT.md) - AnÃ¡lise detalhada com decisÃµes tÃ©cnicas e justificativas

---
## ğŸ¤– Modelos de Machine Learning

**RelatÃ³rio Completo**: [`docs/MODEL_SELECTION.md`](docs/MODEL_SELECTION.md) - AnÃ¡lise comparativa completa de 4 algoritmos + 3 versÃµes XGBoost

### Abordagem Testada: ComparaÃ§Ã£o de 4 Algoritmos
- âœ… **XGBoost v2.1.0** (â­ **RECOMENDADO**): PR-AUC 0.8772, Precision 86.60%, 13 FP
- âœ… **XGBoost v2.0.0** (Grid Search): PR-AUC 0.8847, Precision 85.42%, 14 FP  
- âœ… **XGBoost v1.1.0** (Baseline): PR-AUC 0.8719, Precision 72.27%, 33 FP
- âŒ **Decision Tree**: PR-AUC 0.7680, Precision 24.24%, 250 FP (rejeitado)
- âŒ **LightGBM**: PR-AUC 0.0372, Precision 3.36%, 2,475 FP (falha catastrÃ³fica)
- âŒ **SVM RBF**: PR-AUC 0.5326, Precision 19.50%, 339 FP (muito lento: 15 min)
---

## ğŸ—„ï¸ PostgreSQL Schema

```sql
-- Tabelas do Pipeline
raw_transactions         (284,807 linhas) - CSV original
cleaned_transactions     (284,804 linhas) - Sem outliers
imputed_transactions     (284,804 linhas) - Sem missing
normalized_transactions  (284,804 linhas) - Features normalizadas
engineered_transactions  (284,804 linhas) - Features novas
train_features          (~227,843 linhas) - 80% treino
test_features           (~56,961 linhas)  - 20% teste

-- Tabelas do Webapp
classification_results   - HistÃ³rico de prediÃ§Ãµes do dashboard
simulated_transactions   - TransaÃ§Ãµes geradas pelo simulador

-- Tabelas de Metadados
metrics_history          - MÃ©tricas de modelos ao longo do tempo
trained_models           - Registro de modelos treinados
data_splits              - HistÃ³rico de splits train/test
```

---

## ğŸ¯ Status do Projeto

### âœ… ConcluÃ­do
- [x] Setup PostgreSQL (Docker Compose)
- [x] ConexÃ£o SQLAlchemy testada
- [x] EDA completo com OOP (5 classes)
- [x] Pipeline completo (Steps 01-07)
- [x] Feature Engineering + Selection automatizada
- [x] Treinamento XGBoost v2.1.0 (PR-AUC: 0.8772)
- [x] CLI completo (pipeline/train/tune/predict)
- [x] Backend Flask REST API
- [x] Model Service (singleton)
- [x] Transaction Generator (dados reais)
- [x] Database Models para webapp
- [x] API Endpoints (/simulate, /stats, /history)
- [x] DocumentaÃ§Ã£o completa (EDA, Model Selection, API Endpoints)

### ğŸ”„ Em Progresso
- [ ] Frontend Dashboard (HTML/CSS/JS com Chart.js)

### ğŸ“‹ PrÃ³ximos Passos
- [ ] Dashboard interativo com visualizaÃ§Ãµes
- [ ] VÃ­deo explicativo
- [ ] (Opcional) Kafka streaming para escalabilidade

### SugestÃ£o de Escalabilidade
- Em cenÃ¡rios de alto volume de transaÃ§Ãµes, o sistema pode ser estendido com Apache Kafka para ingestÃ£o distribuÃ­da, mÃºltiplos consumidores e reprocessamento em tempo real

---

## ğŸ“š DocumentaÃ§Ã£o
 
- **[DecisÃµes TÃ©cnicas](docs/DECISOES_TECNICAS.md)**: Justificativas metodolÃ³gicas (ML + Backend)
- **[API Endpoints](docs/API_ENDPOINTS.md)**: DocumentaÃ§Ã£o completa REST API
- **[Pipeline de Tratamento de dados README](src/ml/README.md)**: DocumentaÃ§Ã£o do pipeline

---

## ğŸ”— ReferÃªncias

- **Dataset**: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- **Airflow**: https://airflow.apache.org/
- **Scikit-learn**: https://scikit-learn.org/
- **XGBoost**: https://xgboost.readthedocs.io/
- **PostgreSQL**: https://www.postgresql.org/
--- 

---

## ğŸ‘¥ Autores

**Victor Lucas Santos de Oliveira** - [LinkedIn](https://www.linkedin.com/in/vlso/)

**Adrianny Lelis da Silva** - [LinkedIn](https://www.linkedin.com/in/adriannylelis/)

---
  